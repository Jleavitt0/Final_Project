{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, sum, when, col\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType\n",
    "\n",
    "import plotly.express as px\n",
    "import xarray as xr\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = pd.read_csv(\"D:\\\\downl\\\\final\\\\Loan_default.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_age = bank_df[\"Age\"].mean()\n",
    "print (\"The average age of this dataset is {:.1f}.\".format(avg_age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a spark session\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"bank_info\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Loan_default.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = spark.read.option(\"header\",'True').option('delimiter', ',').csv(path)\n",
    "bank_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = bank_df.count()\n",
    "num_columns = bank_df.columns\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", len(num_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows a hierarchical representation of the DataFrame's schema. Each line represents a column, showing its name and data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the correct colums as a float datatype. \n",
    "columns_to_cast = [\"Age\", \"Income\", \"LoanAmount\", \"CreditScore\", \"MonthsEmployed\", \"NumCreditLines\", \"InterestRate\", \"LoanTerm\", \"DTIRatio\",\"Default\"]\n",
    "\n",
    "for i in columns_to_cast:\n",
    "    bank_df = bank_df.withColumn(i, col(i).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.createOrReplaceTempView(\"bank_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average DTIRatio\n",
    "avg_dti_result = spark.sql(\"SELECT AVG(DTIRatio) as avg_dti FROM bank_table\")\n",
    "avg_dti = avg_dti_result.first()[\"avg_dti\"]\n",
    "\n",
    "# Calculate average Default\n",
    "avg_default_result = spark.sql(\"SELECT AVG(Default) as avg_default FROM bank_table\")\n",
    "avg_default = avg_default_result.first()[\"avg_default\"]\n",
    "\n",
    "# Count records with Default=1\n",
    "count_default_df_result = spark.sql(\"SELECT COUNT(*) as count_default FROM bank_table WHERE Default = 1\")\n",
    "count_default_df = count_default_df_result.first()[\"count_default\"]\n",
    "\n",
    "# Count total records\n",
    "bank_count_result = spark.sql(\"SELECT COUNT(*) as bank_count FROM bank_table\")\n",
    "bank_count = bank_count_result.first()[\"bank_count\"]\n",
    "\n",
    "# Display the results\n",
    "print(f\"The total number of records in this dataset is: {bank_count}\")\n",
    "print(f\"The average Debt to income ratio across the data set is: {avg_dti}\")\n",
    "print(f\"The average default rate is: {avg_default}, meaning of the {bank_count} records {count_default_df} defaulted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table showing the average Debt-To-Income Ratio and Default\n",
    "query = \"\"\"\n",
    "SELECT COUNT(*) AS TotalRecords, AVG(DTIRatio) AS AVGDTIRatio, AVG(Default) AS AVGDefault\n",
    "FROM bank_table\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to select ages over 55 and their interest rate\n",
    "query = \"\"\"\n",
    "SELECT Age, InterestRate\n",
    "FROM bank_table\n",
    "WHERE Age > 55\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Interest Rate for people over 55\n",
    "query = \"\"\"\n",
    "SELECT AVG(InterestRate) AS AvgInterestRateOver55\n",
    "FROM bank_table\n",
    "WHERE Age > 55\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Interest Rate for people under 55\n",
    "query = \"\"\"\n",
    "SELECT AVG(InterestRate) AS AvgInterestRateUnder55\n",
    "FROM bank_table\n",
    "WHERE Age < 55\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to analyze the impact of interest rates on default rates\n",
    "query = \"\"\"\n",
    "SELECT InterestRate, AVG(InterestRate) AS AvgInterestRate, AVG(Default) AS AvgDefaultRate\n",
    "FROM bank_table\n",
    "GROUP BY InterestRate\n",
    "ORDER BY AvgDefaultRate DESC\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and show the results\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Credit Lines VS Default\n",
    "query = \"\"\"\n",
    "SELECT NumCreditLines, AVG(Default) AS AvgDefaultRate\n",
    "FROM bank_table\n",
    "GROUP BY NumCreditLines\n",
    "ORDER BY AvgDefaultRate DESC\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and show the results\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan Term VS Default\n",
    "query = \"\"\"\n",
    "SELECT LoanTerm, AVG(Default) AS AvgDefaultRate\n",
    "FROM bank_table\n",
    "GROUP BY LoanTerm\n",
    "ORDER BY AvgDefaultRate DESC\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and show the results\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting PySpark DataFrame to Pandas DataFrame\n",
    "df = spark.sql(\"SELECT * FROM bank_table\")\n",
    "bank_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_records_df = pd.DataFrame(bank_df[(bank_df['Default']==1)])\n",
    "non_default_records_df = pd.DataFrame(bank_df[(bank_df['Default']!=1)])\n",
    "\n",
    "avg_default_dti = default_records_df['DTIRatio'].mean().__round__(5)\n",
    "avg_non_default_dti = non_default_records_df['DTIRatio'].mean().__round__(5)\n",
    "\n",
    "default_credit_lines = default_records_df['NumCreditLines'].mean().__round__(2)\n",
    "non_default_credit_lines = non_default_records_df['NumCreditLines'].mean().__round__(2)\n",
    "\n",
    "default_avg_interest = default_records_df['InterestRate'].mean().__round__(3)\n",
    "non_default_avg_interest = non_default_records_df['InterestRate'].mean().__round__(3)\n",
    "\n",
    "default_avg_income = default_records_df['Income'].mean().__round__(2)\n",
    "non_default_avg_income = non_default_records_df['Income'].mean().__round__(2)\n",
    "\n",
    "\n",
    "print(f'''Of the records for which the loan was defaulted, the average debt to income ratio is: {avg_default_dti}.'''),\n",
    "print(f'''Of the records for which the loan was defaulted, the average interest rate is: {default_avg_interest}%.'''),\n",
    "print(f'''Of the records for which the loan was defaulted, the average number of credit lines is: {default_credit_lines}.'''),\n",
    "print(f'''Of the records for which the loan was defaulted, the average income is: ${default_avg_income}.'''),\n",
    "\n",
    "print(\"  \"),\n",
    "\n",
    "print(f'''Of the records for which the loan did NOT default, the average debt to income ratio is: {avg_non_default_dti}.'''),\n",
    "print(f'''Of the records for which the loan did NOT default the average interest rate is: {non_default_avg_interest}%.'''),\n",
    "print(f'''Of the records for which the loan did NOT default, the average number of credit lines is: {non_default_credit_lines}.'''),\n",
    "print(f'''Of the records for which the loan did NOT default, the average income is: ${non_default_avg_income}.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_records_df.to_csv(\"default_records.csv\")\n",
    "non_default_records_df.to_csv(\"non_default_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1 = px.scatter(x=default_records_df['Income'], y=default_records_df['CreditScore'],\n",
    "                    labels={'x':'Income',\n",
    "                            'y':'Credit Score'},\n",
    "                    title=\"Income vs Credit Score for Defaulted Loans\")\n",
    "plot_1.update_traces(marker_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#requires pip install -U kaleido\n",
    "output_directory = 'Images'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "plot_1.write_image('Images/Defaulted_Income_vs_Credit.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2 = px.scatter(x=default_records_df['Income'], y=default_records_df['InterestRate'],\n",
    "                    labels={'x':'Income',\n",
    "                            'y':'Interest Rate'},\n",
    "                    title=\"Income vs Interest Rate for Defaulted Loans\")\n",
    "plot_2.update_traces(marker_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2.write_image('Images/Defaulted_Income_vs_Interest_Rate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3 = px.scatter(x=non_default_records_df['Income'], y=non_default_records_df['CreditScore'],\n",
    "                    labels={'x':'Income',\n",
    "                            'y':'Credit Score'},\n",
    "                    title=\"Non-Defaulted Loans Income vs Credit Score\")\n",
    "plot_3.update_traces(marker_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3.write_image('Images/non_Defaulted_Income_vs_credit_score.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_4 = px.scatter(x=non_default_records_df['Income'], y=non_default_records_df['InterestRate'],\n",
    "                    labels={'x':'Income',\n",
    "                            'y':'Interest Rate'},\n",
    "                    title=\"Non-Defaulted Loans Income vs Interest Rate\")\n",
    "plot_4.update_traces(marker_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_4.write_image('Images/non_Defaulted_Income_vs_interest_rate.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning/Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an index column (#)\n",
    "bank_df.insert(0,'#',range(len(bank_df)))\n",
    "bank_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial ID columns\n",
    "bank_df = bank_df.drop(columns=['Education','EmploymentType','MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner'])\n",
    "bank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"D:\\\\downl\\\\final\\\\Loan_default.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the LoanID column as it is not needed for modeling\n",
    "data = data.drop(columns=['LoanID'])\n",
    "\n",
    "# Convert categorical variables to numerical\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['Default'])\n",
    "y = data['Default']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a random forest classifier\n",
    "model = RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=150)\n",
    "\n",
    "# Train the model on the resampled data\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the resampled data\n",
    "model.fit(X_resampled, y_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred_adjusted = (y_pred_proba > 0.7).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "report = classification_report(y_test, y_pred_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Plot feature importances\n",
    "feature_importance = model.feature_importances_\n",
    "sorted = feature_importance.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(feature_importance)), feature_importance[sorted], align='center')\n",
    "plt.yticks(range(len(X.columns)), X.columns[sorted])\n",
    "plt.xlabel('Influence')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Model finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
